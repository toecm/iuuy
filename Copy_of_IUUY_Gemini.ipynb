{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toecm/iuuy/blob/main/Copy_of_IUUY_Gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqaCSKWXU9Zp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bae1a1da-f3b0-4ff0-a460-06cd7f7dacf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# 📦 Install dependencies\n",
        "!pip install -q openai-whisper gradio ffmpeg-python requests pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpuWVndfVKem",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1174c99-5842-42df-9eb4-14b0bda12ed2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: GEMINI_MODEL_NAME set to: gemini-1.5-flash\n"
          ]
        }
      ],
      "source": [
        "# 📚 Imports\n",
        "import whisper\n",
        "import gradio as gr\n",
        "import hashlib\n",
        "import pandas as pd\n",
        "import os\n",
        "import datetime\n",
        "import requests\n",
        "import json\n",
        "import tempfile\n",
        "\n",
        "\n",
        "# 🔐 Set API keys and endpoints\n",
        "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", \"AIzaSyCYdHgnxJpb5FUJ3x96YB0VcRrYf67lgMg\")\n",
        "PINATA_JWT = os.getenv(\"PINATA_JWT\", \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VySW5mb3JtYXRpb24iOnsiaWQiOiI4YWE0YzYwZi0yZmYwLTQyOWItYWZkMy1jNjMxNWFhOTliMjgiLCJlbWFpbCI6Im9ueWVkaWthLm1iYUBnbWFpbC5jb20iLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwicGluX3BvbGljeSI6eyJyZWdpb25zIjpbeyJkZXNpcmVkUmVwbGljYXRpb25Db3VudCI6MSwiaWQiOiJGUkExIn0seyJkZXNpcmVkUmVwbGljYXRpb25Db3VntCI6MSwiaWQiOiJOWUMxIn1dLCJ2ZXJzaW9uIjoxfSwibWZhX2VuYWJsZWQiOmZhbHNlLCJzdGF0dXMiOiJBQ1RJVkUifSwiYXV0aGVudGljYXRpb25UeXBlIjoic2NvcGVkS2V5Iiwic2NvcGVkS2V5S2V5IjoiM2I2ZDJhMDVkM2QwMjllNGQyNWIiLCJzY29wZWRLZXlTZWNyZXQiOiI3OWU3Mzc3MDZkYWY2YWI3Mjk2MTA0NjRmY2YyY2QwZjI1OTUwOWZiZTU5NWUzOWUzOTZkYjQ5OTNlNTk0YjMwIiwiZXhwIjoxNzg0MzgzNjIxfQ.lGQ8MDBSuD3XCTy45jMZHX4vivaZ--QuawauSsoWbO8\")\n",
        "\n",
        "# Check keys early and stop execution if not set (optional early fails)\n",
        "#if GEMINI_API_KEY == \"YOUR_GEMINI_API_KEY_PLACEHOLDER\":\n",
        "#    print(\"Warning: Gemini API key placeholder detected. Replace with your actual key!\")\n",
        "#if PINATA_JWT == \"YOUR_PINATA_JWT_PLACEHOLDER\":\n",
        "#    print(\"Warning: Pinata JWT placeholder detected. Replace with your actual token!\")\n",
        "\n",
        "\n",
        "# 🎯 Gemini API config (update with actual endpoint & model)\n",
        "GEMINI_MODEL_NAME = \"gemini-1.5-flash\"  # Current model!\n",
        "GEMINI_API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/MODEL_NAME:generateContent\"  # Confirm current endpoint!\n",
        "\n",
        "# Added for debugging: Confirm that GEMINI_MODEL_NAME is set\n",
        "print(f\"DEBUG: GEMINI_MODEL_NAME set to: {GEMINI_MODEL_NAME}\")\n",
        "\n",
        "\n",
        "# 🎙️ Load Whisper model\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "\n",
        "# 📁 Ensure belief file exists\n",
        "belief_file = \"dialect_beliefs.csv\"\n",
        "if not os.path.exists(belief_file):\n",
        "    pd.DataFrame(columns=[\"Dialect\", \"Correct (α)\", \"Incorrect (β)\"]).to_csv(belief_file, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "iEPHwnFHOpx6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7ab84c8-eb36-43b5-bfbc-91a734171467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PM0TSvXxVLxf"
      },
      "outputs": [],
      "source": [
        "# 🎧 Audio Transcription\n",
        "def transcribe_audio(audio_path):\n",
        "    \"\"\"\n",
        "    Transcribes audio from a given file path using the Whisper model.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        result = model.transcribe(audio_path)\n",
        "        return result['text']\n",
        "    except Exception as e:\n",
        "        print(f\"Error during audio transcription: {e}\")\n",
        "        return None\n",
        "\n",
        "# 🌍 Dialect Detector\n",
        "def detect_dialect(text):\n",
        "    \"\"\"\n",
        "    Detects dialect features in the transcribed text based on a predefined sample dataset.\n",
        "    \"\"\"\n",
        "    text_lower = text.lower()\n",
        "    for _, row in df_sample.iterrows():\n",
        "        if row['Utterance'].lower() in text_lower:\n",
        "            return row['Dialect'], f\"Matched: '{row['Utterance']}' → '{row['Clarification']}'\"\n",
        "    return \"Standard English\", \"No flagged dialect features\"\n",
        "\n",
        "# 💬 Clarification Agent with improved OpenAI API key check\n",
        "def clarify_phrase_gemini(phrase):\n",
        "    \"\"\"\n",
        "    Uses the Google Gemini API to clarify a given phrase.\n",
        "    The API key is passed as a query parameter, and the request body\n",
        "    follows the Generative Language API's 'generateContent' format.\n",
        "    \"\"\"\n",
        "    if not GEMINI_API_KEY or GEMINI_API_KEY == \"YOUR_GEMINI_API_KEY_PLACEHOLDER\":\n",
        "        return \"❌ Clarification failed: Gemini API key not set or invalid.\"\n",
        "\n",
        "    current_gemini_api_url = f\"https://generativelanguage.googleapis.com/v1beta/models/{GEMINI_MODEL_NAME}:generateContent?key={GEMINI_API_KEY}\"\n",
        "\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    # The prompt is structured within the 'contents' array with 'role' and 'parts'.\n",
        "    # The system instruction is included as part of the user's initial prompt.\n",
        "    data = {\n",
        "        \"contents\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"parts\": [\n",
        "                    {\"text\": (\n",
        "                        \"You are a multicultural language assistant. \"\n",
        "                        \"Your job is to explain English phrases including dialect-specific expressions, idioms, or culturally influenced phrases. \"\n",
        "                        \"Keep the speaker’s tone and context intact.\\n\\n\"\n",
        "                        f\"Clarify this sentence: \\\"{phrase}\\\"\"\n",
        "                    )}\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "        # Optional: Add generationConfig for temperature, topK, topP, etc.\n",
        "        # \"generationConfig\": {\n",
        "        #     \"temperature\": 0.7,\n",
        "        #     \"topK\": 40,\n",
        "        #     \"topP\": 0.95\n",
        "        # }\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(current_gemini_api_url, headers=headers, json=data)\n",
        "        response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "        response_json = response.json()\n",
        "\n",
        "        # Parse the response based on the Gemini API's 'generateContent' structure\n",
        "        if 'candidates' in response_json and len(response_json['candidates']) > 0:\n",
        "            first_candidate = response_json['candidates'][0]\n",
        "            if 'content' in first_candidate and 'parts' in first_candidate['content'] and len(first_candidate['content']['parts']) > 0:\n",
        "                return first_candidate['content']['parts'][0]['text']\n",
        "            else:\n",
        "                return f\"❌ Gemini API response missing expected content structure in candidate: {response_json}\"\n",
        "        else:\n",
        "            # Handle cases where no candidates are returned (e.g., due to safety settings)\n",
        "            if 'promptFeedback' in response_json:\n",
        "                feedback = response_json['promptFeedback']\n",
        "                if 'blockReason' in feedback:\n",
        "                    return f\"❌ Gemini API blocked prompt: {feedback.get('blockReason', 'Unknown reason')}. Safety ratings: {feedback.get('safetyRatings', 'N/A')}\"\n",
        "            return f\"❌ Gemini API response missing candidates: {response_json}\"\n",
        "\n",
        "    except requests.exceptions.HTTPError as http_err:\n",
        "        return f\"❌ Gemini API HTTP error {http_err.response.status_code}: {http_err.response.text}\"\n",
        "    except requests.exceptions.ConnectionError as conn_err:\n",
        "        return f\"❌ Gemini API connection error: {conn_err}\"\n",
        "    except requests.exceptions.Timeout as timeout_err:\n",
        "        return f\"❌ Gemini API timeout error: {timeout_err}\"\n",
        "    except requests.exceptions.RequestException as req_err:\n",
        "        return f\"❌ Gemini API request error: {req_err}\"\n",
        "    except json.JSONDecodeError as json_err:\n",
        "        return f\"❌ Gemini API response JSON decode error: {json_err}. Response: {response.text}\"\n",
        "    except Exception as e:\n",
        "        return f\"❌ An unexpected error occurred during Gemini API call: {str(e)}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEvoPVc6VTKX"
      },
      "outputs": [],
      "source": [
        "# 📤 Store Feedback to IPFS via Pinata with improved JWT check\n",
        "def store_to_ipfs_pinata(data_dict):\n",
        "    \"\"\"\n",
        "    Stores data to IPFS via Pinata.\n",
        "    Requires a valid Pinata JWT.\n",
        "    \"\"\"\n",
        "    if not PINATA_JWT or len(PINATA_JWT.split('.')) != 3: # Basic JWT format check\n",
        "        return \"❌ Upload failed: Pinata JWT not set or invalid.\"\n",
        "\n",
        "    url = \"https://api.pinata.cloud/pinning/pinFileToIPFS\"\n",
        "    headers = {\"Authorization\": f\"Bearer {PINATA_JWT}\"}\n",
        "\n",
        "    # Create a temporary JSON file to upload\n",
        "    temp_file_path = None\n",
        "    try:\n",
        "        with tempfile.NamedTemporaryFile(\"w+\", suffix=\".json\", delete=False) as temp_file:\n",
        "            json.dump(data_dict, temp_file)\n",
        "            temp_file_path = temp_file.name # Store path for later cleanup\n",
        "\n",
        "        with open(temp_file_path, \"rb\") as f:\n",
        "            files = {\"file\": (os.path.basename(temp_file_path), f)}\n",
        "            response = requests.post(url, headers=headers, files=files)\n",
        "            response.raise_for_status() # Raise HTTPError for bad responses\n",
        "\n",
        "        res_json = response.json()\n",
        "        if \"IpfsHash\" in res_json:\n",
        "            return res_json[\"IpfsHash\"]\n",
        "        else:\n",
        "            return f\"❌ Upload failed: {res_json.get('error', 'Unknown Pinata error')}\"\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"❌ Pinata API request failed: {e}\"\n",
        "    except json.JSONDecodeError as e:\n",
        "        return f\"❌ Pinata response JSON decode error: {e}. Response: {response.text}\"\n",
        "    except Exception as e:\n",
        "        return f\"❌ An unexpected error occurred during Pinata upload: {str(e)}\"\n",
        "    finally:\n",
        "        if temp_file_path and os.path.exists(temp_file_path):\n",
        "            os.remove(temp_file_path) # Clean up the temporary file\n",
        "\n",
        "# 🧠 Feedback Logger\n",
        "def log_feedback(transcript, suggestion, dialect, feedback, comment):\n",
        "    \"\"\"\n",
        "    Logs feedback, including transcription, suggestion, dialect, user feedback,\n",
        "    comments, and a hash, then attempts to store it on IPFS via Pinata.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Ensure all inputs are strings for consistent hashing and logging\n",
        "        safe_transcript = transcript or \"\"\n",
        "        safe_suggestion = suggestion or \"\"\n",
        "        safe_feedback = feedback or \"\"\n",
        "        safe_comment = comment or \"\"\n",
        "        safe_dialect = dialect or \"Unknown\"\n",
        "\n",
        "        feedback_hash = hashlib.sha256((safe_transcript + safe_suggestion + safe_feedback + safe_comment).encode()).hexdigest()\n",
        "        now = datetime.datetime.now().isoformat()\n",
        "\n",
        "        row = {\n",
        "            \"Timestamp\": now,\n",
        "            \"Transcript\": safe_transcript,\n",
        "            \"Suggestion\": safe_suggestion,\n",
        "            \"Dialect\": safe_dialect,\n",
        "            \"Feedback\": safe_feedback,\n",
        "            \"Comment\": safe_comment,\n",
        "            \"Hash\": feedback_hash\n",
        "        }\n",
        "\n",
        "        ipfs_cid = \"N/A (IPFS upload skipped/failed)\" # Default in case of IPFS error\n",
        "        try:\n",
        "            ipfs_cid = store_to_ipfs_pinata(row)\n",
        "        except Exception as ipfs_error:\n",
        "            ipfs_cid = f\"❌ IPFS Error: {str(ipfs_error)}\"\n",
        "\n",
        "        row[\"IPFS_CID\"] = ipfs_cid # Add CID to the row before saving to CSV\n",
        "\n",
        "        df = pd.DataFrame([row])\n",
        "        if not os.path.exists(\"feedback_log.csv\"):\n",
        "            df.to_csv(\"feedback_log.csv\", index=False)\n",
        "        else:\n",
        "            df.to_csv(\"feedback_log.csv\", mode='a', header=False, index=False)\n",
        "\n",
        "        return f\"✅ Feedback stored.\\n- Hash: `{feedback_hash}`\\n- IPFS CID: `{ipfs_cid}`\"\n",
        "    except Exception as e:\n",
        "        return f\"❌ Logging error: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZU-c6k4qVc31"
      },
      "outputs": [],
      "source": [
        "# 🔁 Full Pipeline\n",
        "def full_pipeline(audio_path, feedback, comment):\n",
        "    \"\"\"\n",
        "    Orchestrates the full pipeline: transcribe audio, detect dialect,\n",
        "    clarify phrase using Gemini, and log user feedback.\n",
        "    \"\"\"\n",
        "    if not audio_path:\n",
        "        return \"❌ No audio file uploaded.\"\n",
        "\n",
        "    transcript = None\n",
        "    dialect = \"Unknown\"\n",
        "    flagged = \"N/A\"\n",
        "    suggestion = \"❗ Clarification unavailable.\"\n",
        "    log_msg = \"❗ Feedback logging skipped due to initial errors.\"\n",
        "\n",
        "    try:\n",
        "        # 1. Transcribe Audio\n",
        "        transcript = transcribe_audio(audio_path)\n",
        "        if not transcript or transcript.strip() == \"\":\n",
        "            return \"❌ Transcription failed. Please try with clearer audio or a different audio file.\"\n",
        "\n",
        "        # 2. Detect Dialect\n",
        "        dialect, flagged = detect_dialect(transcript)\n",
        "\n",
        "        # 3. Clarify Phrase using Gemini\n",
        "        suggestion = clarify_phrase_gemini(transcript)\n",
        "\n",
        "        # 4. Log Feedback\n",
        "        log_msg = log_feedback(transcript, suggestion, dialect, feedback, comment)\n",
        "\n",
        "        return f\"\"\"### Transcript\n",
        "{transcript}\n",
        "\n",
        "\n",
        "**Detected Dialect:** {dialect}\n",
        "**Flagged Phrase:** {flagged}\n",
        "\n",
        "\n",
        "**Clarified Suggestion:** {suggestion}\n",
        "\n",
        "\n",
        "{log_msg}\n",
        "\"\"\"\n",
        "    except Exception as e:\n",
        "        # Catch any unexpected errors in the pipeline\n",
        "        return f\"❌ An unexpected error occurred during processing: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "gbcYO6lgVpJ1",
        "outputId": "f4d1bb9d-0e30-4bbe-b342-1165c18ff6b8"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://deee4d269742791250.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://deee4d269742791250.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://deee4d269742791250.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# 🚀 Gradio UI\n",
        "gr.Interface(\n",
        "    fn=full_pipeline,\n",
        "    inputs=[\n",
        "        gr.Audio(type=\"filepath\", label=\"Upload Audio (.wav or .mp3)\"),\n",
        "        gr.Radio([\"👍 Yes\", \"👎 No\", \"🤔 Needs Review\"], label=\"Was this clarification helpful?\"),\n",
        "        gr.Textbox(lines=2, placeholder=\"Optional feedback comment...\", label=\"Comments\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"I Understand Understand You – MAS Prototype\",\n",
        "    description=\"Upload an audio file. The system will transcribe, detect dialects, clarify meanings, and log your feedback.\"\n",
        ").launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ikMsZSyYId9"
      },
      "outputs": [],
      "source": [
        "# This cell is no longer needed."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNiAzRTHvNZ8axmP5B3Ywdt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}