{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "authorship_tag": "ABX9TyN9hMgW0e3wc4MM7j+zB8wp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toecm/iuuy/blob/main/IUUY_beta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📦 Step 1: Install Dependencies"
      ],
      "metadata": {
        "id": "2jOYGLboMLik"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M-g9gBUwFc3T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a7d5301-3c14-49c5-c727-59ac1a2ff400"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/803.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/803.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m798.7/803.2 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.0/176.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.9/73.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.6/324.6 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.4/155.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.6/95.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.7/146.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.9/319.9 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m125.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.0/73.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.3/491.3 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m120.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.8/241.8 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.4/224.4 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m355.6/355.6 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q openai-whisper rapidfuzz pandas gradio datasets transformers torchaudio torch librosa pydub ffmpeg-python jiwer google-generativeai python-dotenv requests yt-dlp soundfile\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📚 Step 2: Imports & API Configs"
      ],
      "metadata": {
        "id": "3ypxkdXtD9bJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import whisper\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from pydub import AudioSegment\n",
        "import pandas as pd\n",
        "from rapidfuzz import fuzz\n",
        "import google.generativeai as genai\n",
        "from datasets import load_dataset, Audio\n",
        "import gradio as gr\n",
        "from dotenv import load_dotenv\n",
        "import yt_dlp\n",
        "import tempfile\n",
        "\n",
        "# Load environment variables for API keys\n",
        "load_dotenv()\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
        "PINATA_JWT = os.getenv(\"PINATA_JWT\")\n",
        "\n",
        "# Configure Google Generative AI and initialize Gemini model\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "gemini_model = genai.GenerativeModel(\"gemini-2.5-flash\")\n"
      ],
      "metadata": {
        "id": "4vzj_roSFuu4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🤖 AGENT 1: Ingestion & Diarization"
      ],
      "metadata": {
        "id": "r3qG60RDXTw9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ea7126d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c18c281-534b-4778-b261-fbc85de85fce"
      },
      "source": [
        "class AgentInput:\n",
        "    def __init__(self, whisper_model_size=\"small\"):\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        # Load Whisper ASR model\n",
        "        self.whisper_model = whisper.load_model(whisper_model_size, device=device)\n",
        "\n",
        "    def get_audio_from_youtube(self, url):\n",
        "        opts = {'format': 'bestaudio', 'outtmpl': 'external_audio.%(ext)s'}\n",
        "        with yt_dlp.YoutubeDL(opts) as ydl:\n",
        "            info = ydl.extract_info(url, download=True)\n",
        "            fn = ydl.prepare_filename(info)\n",
        "        return fn, info.get('title', 'No Title')\n",
        "\n",
        "    def sample_audio_dataset(self, dataset_name, n=3):\n",
        "        ds = load_dataset(dataset_name, split=\"train\").cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "        return ds.shuffle(seed=42).select(range(n))\n",
        "\n",
        "    def transcribe(self, audio_path, language=\"en\"):\n",
        "        \"\"\"\n",
        "        Transcribe audio forcing specified language to reduce detection time.\n",
        "        \"\"\"\n",
        "        result = self.whisper_model.transcribe(audio_path, language=language)\n",
        "        return [\n",
        "            {\"speaker\": \"Speaker\", \"text\": seg[\"text\"], \"start\": seg[\"start\"], \"end\": seg[\"end\"]}\n",
        "            for seg in result[\"segments\"]\n",
        "        ]\n",
        "\n",
        "    def trim_audio(self, path, s, e):\n",
        "        aud = AudioSegment.from_file(path)\n",
        "        clip = aud[s*1000:e*1000]\n",
        "        out = f\"{os.path.splitext(path)[0]}_trimmed_{s}_{e}.wav\"\n",
        "        clip.export(out, format=\"wav\")\n",
        "        return out\n",
        "\n",
        "agent_input = AgentInput()\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 461M/461M [00:17<00:00, 27.7MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#🔍 🧠 AGENT 2: Interpretation (Dialect Detection + Clarification)"
      ],
      "metadata": {
        "id": "17Ce0eoNM3nf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentInterpretation:\n",
        "    def __init__(self, data_path=\"dialect_data.csv\", fuzzy_threshold=80):\n",
        "        self.data_path = data_path\n",
        "        self.fuzzy_threshold = fuzzy_threshold\n",
        "        self._load_data()\n",
        "\n",
        "    def _load_data(self):\n",
        "        default_data = [\n",
        "            {\"Utterance\": \"I am seeing her tomorrow.\", \"Dialect\": \"Nigerian English\", \"Clarification\": \"I will meet her tomorrow.\"},\n",
        "            {\"Utterance\": \"I didn’t see nobody at the party.\", \"Dialect\": \"Nigerian English\", \"Clarification\": \"I didn’t see anyone at the party.\"},\n",
        "            {\"Utterance\": \"You are coming, abi?\", \"Dialect\": \"Nigerian English\", \"Clarification\": \"You are coming, right?\"},\n",
        "            {\"Utterance\": \"I beg, let's go nau.\", \"Dialect\": \"Nigerian English\", \"Clarification\": \"Please, let’s just leave already.\"},\n",
        "            {\"Utterance\": \"I beg, don't be angry.\", \"Dialect\": \"Nigerian English\", \"Clarification\": \"Please, don’t be upset.\"},\n",
        "            {\"Utterance\": \"How are you, jare?\", \"Dialect\": \"Nigerian English\", \"Clarification\": \"How are you, dear?\"},\n",
        "            {\"Utterance\": \"I go to office at 9 AM.\", \"Dialect\": \"Korean English (Konglish)\", \"Clarification\": \"I go to the office at 9 AM.\"},\n",
        "            {\"Utterance\": \"It’s raining? I bring umbrella?\", \"Dialect\": \"Korean English (Konglish)\", \"Clarification\": \"Is it raining? Should I bring an umbrella?\"},\n",
        "            {\"Utterance\": \"She call me yesterday but I couldn’t answer.\", \"Dialect\": \"Korean English (Konglish)\", \"Clarification\": \"She called me yesterday, but I couldn’t answer.\"},\n",
        "            {\"Utterance\": \"I will prepone the meeting to Monday.\", \"Dialect\": \"Indian English\", \"Clarification\": \"I will move the meeting to Monday.\"},\n",
        "            {\"Utterance\": \"Can you give me some more time?\", \"Dialect\": \"Indian English\", \"Clarification\": \"Could you give me a bit more time?\"},\n",
        "            {\"Utterance\": \"She is a very good girl only.\", \"Dialect\": \"Indian English\", \"Clarification\": \"She is a very good girl indeed.\"},\n",
        "            {\"Utterance\": \"No issues, yaar.\", \"Dialect\": \"Indian English\", \"Clarification\": \"It’s fine, friend.\"},\n",
        "            {\"Utterance\": \"No issues, we’ll manage.\", \"Dialect\": \"Indian English\", \"Clarification\": \"It’s okay, we will manage.\"},\n",
        "            {\"Utterance\": \"No issues, carry on.\", \"Dialect\": \"Indian English\", \"Clarification\": \"It’s fine, continue.\"},\n",
        "            {\"Utterance\": \"I’m gonna grab some coffee.\", \"Dialect\": \"American English\", \"Clarification\": \"I’m going to get some coffee.\"},\n",
        "            {\"Utterance\": \"Do you wanna catch a movie later?\", \"Dialect\": \"American English\", \"Clarification\": \"Do you want to see a movie later?\"},\n",
        "            {\"Utterance\": \"He likes basketball, right?\", \"Dialect\": \"American English\", \"Clarification\": \"He likes basketball, doesn’t he?\"},\n",
        "            {\"Utterance\": \"I’m going on holiday next week.\", \"Dialect\": \"UK English\", \"Clarification\": \"I’m going on vacation next week.\"},\n",
        "            {\"Utterance\": \"Could you pop round for tea?\", \"Dialect\": \"UK English\", \"Clarification\": \"Could you come over for tea?\"},\n",
        "            {\"Utterance\": \"She’s got a new flat.\", \"Dialect\": \"UK English\", \"Clarification\": \"She has a new apartment.\"},\n",
        "        ]\n",
        "        if os.path.exists(self.data_path):\n",
        "            self.df = pd.read_csv(self.data_path)\n",
        "        else:\n",
        "            self.df = pd.DataFrame(default_data)\n",
        "            self.df.to_csv(self.data_path, index=False)\n",
        "\n",
        "    def detect_dialect(self, text):\n",
        "        best, score = None, 0\n",
        "        for _, r in self.df.iterrows():\n",
        "            s = fuzz.ratio(text.lower(), r[\"Utterance\"].lower())\n",
        "            if s > score:\n",
        "                best, score = r, s\n",
        "        if score >= self.fuzzy_threshold:\n",
        "            return best[\"Dialect\"], best[\"Clarification\"]\n",
        "        return \"Unknown\", \"No dialect feature detected.\"\n",
        "\n",
        "    def clarify(self, phrase):\n",
        "        return gemini_model.generate_content(f\"Explain clearly: '{phrase}'\").text\n",
        "\n",
        "    def finetune_on_feedback(self, feedback):\n",
        "        # placeholder for RLHF update\n",
        "        pass\n",
        "\n",
        "agent_interpretation = AgentInterpretation()\n"
      ],
      "metadata": {
        "id": "xsV2whl6M9mq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧑‍💻 AGENT 3: Output - UX Display (via Gradio)"
      ],
      "metadata": {
        "id": "Fq73G0vCNY1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentUX:\n",
        "    pass\n",
        "\n",
        "agent_ux = AgentUX()\n"
      ],
      "metadata": {
        "id": "xWPgvCWzNdQe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔐 AGENT 4: Trust (Bayesian Update + IPFS Logging)"
      ],
      "metadata": {
        "id": "vSxcVJD4NiF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentTrustLearning:\n",
        "    def __init__(self, pinata_jwt=None):\n",
        "        self.pinata_jwt = pinata_jwt\n",
        "        self.belief_path = \"belief_scores.csv\"\n",
        "        # Initialize belief file if missing\n",
        "        if not os.path.exists(self.belief_path):\n",
        "            pd.DataFrame(columns=[\"Dialect\", \"α_correct\", \"β_incorrect\"]).to_csv(self.belief_path, index=False)\n",
        "        # In-memory feedback buffer for RLHF\n",
        "        self.feedback_buffer = []\n",
        "\n",
        "    def update_beliefs(self, dialect, positive: bool):\n",
        "        df = pd.read_csv(self.belief_path)\n",
        "        if dialect in df[\"Dialect\"].values:\n",
        "            idx = df.index[df[\"Dialect\"] == dialect][0]\n",
        "            df.at[idx, \"α_correct\"] += int(positive)\n",
        "            df.at[idx, \"β_incorrect\"] += int(not positive)\n",
        "        else:\n",
        "            new_row = {\"Dialect\": dialect, \"α_correct\": int(positive), \"β_incorrect\": int(not positive)}\n",
        "            df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
        "        df.to_csv(self.belief_path, index=False)\n",
        "\n",
        "    def log_feedback(self, data: dict) -> str:\n",
        "        headers = {\"Authorization\": f\"Bearer {self.pinata_jwt}\"}\n",
        "        try:\n",
        "            res = requests.post(\n",
        "                \"https://api.pinata.cloud/pinning/pinJSONToIPFS\", headers=headers, json=data\n",
        "            )\n",
        "            return res.json().get(\"IpfsHash\", \"No CID\")\n",
        "        except Exception as e:\n",
        "            return f\"❌ IPFS Error: {e}\"\n",
        "\n",
        "    def get_belief_score(self, dialect: str) -> float:\n",
        "        df = pd.read_csv(self.belief_path)\n",
        "        row = df[df[\"Dialect\"] == dialect]\n",
        "        if not row.empty:\n",
        "            alpha = int(row[\"α_correct\"].iloc[0])\n",
        "            beta = int(row[\"β_incorrect\"].iloc[0])\n",
        "            total = alpha + beta\n",
        "            return round(alpha / total, 2) if total > 0 else 0.5\n",
        "        return 0.5\n",
        "\n",
        "    def record_feedback(self, feedback: dict, positive: bool = True):\n",
        "        # Store in buffer\n",
        "        self.feedback_buffer.append((feedback, positive))\n",
        "        # Update local beliefs\n",
        "        self.update_beliefs(feedback.get(\"Dialect\", \"Unknown\"), positive)\n",
        "        # Log to IPFS\n",
        "        cid = self.log_feedback(feedback)\n",
        "        return cid\n",
        "\n",
        "    def audit_performance(self, interpretation_agent):\n",
        "        # Apply RLHF: fine-tune interpretation based on buffered feedback\n",
        "        for feedback, positive in self.feedback_buffer:\n",
        "            # update the agent with each feedback instance\n",
        "            interpretation_agent.finetune_on_feedback(feedback)\n",
        "        # Clear buffer after applying\n",
        "        self.feedback_buffer.clear()\n",
        "        return True"
      ],
      "metadata": {
        "id": "MQ0A3W55Ngvx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#🔄 External Import Function (YouTube or HuggingFace)"
      ],
      "metadata": {
        "id": "HNWRuFeu9KsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tempfile, requests\n",
        "\n",
        "def import_external_audio(source_type, url, sample_count=3, progress=gr.Progress()):\n",
        "    \"\"\"\n",
        "    Imports audio from YouTube or HuggingFace URLs and returns:\n",
        "    1. audio_path (for playing)\n",
        "    2. full transcript (for detailed view)\n",
        "    3. state path (to feed into the pipeline)\n",
        "    4. status message\n",
        "    5. title\n",
        "    \"\"\"\n",
        "    progress(0, desc=\"Importing...\")\n",
        "    if not url:\n",
        "        return None, \"\", None, \"❌ URL is required\", \"No Title\"\n",
        "\n",
        "    audio_path = None\n",
        "    transcript = \"\"\n",
        "    status_msg = \"\"\n",
        "    title = \"\"\n",
        "\n",
        "    if source_type == \"YouTube\":\n",
        "        audio_path, title = agent1.get_audio_from_youtube(url)\n",
        "        if audio_path:\n",
        "            progress(0.3, desc=\"Transcribing...\")\n",
        "            # Get full transcription segments\n",
        "            segments = agent1.transcribe(audio_path)\n",
        "            # Concatenate texts\n",
        "            transcript = \" \".join(seg[\"text\"] for seg in segments)\n",
        "            # Preview first 500 chars\n",
        "            preview = (transcript[:500] + \"...\") if len(transcript) > 500 else transcript\n",
        "            status_msg = f\"✅ Imported: {preview}\"\n",
        "        else:\n",
        "            status_msg = \"❌ YouTube download failed\"\n",
        "    elif source_type == \"HuggingFace\":\n",
        "        try:\n",
        "            samples = agent1.sample_audio_dataset(url, n=int(sample_count))\n",
        "            audio_array = samples[0][\"audio\"][\"array\"]\n",
        "            transcript = samples[0][\"text\"]\n",
        "            # Write to temp file\n",
        "            temp_path = tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False).name\n",
        "            sf.write(temp_path, audio_array, 16000)\n",
        "            audio_path = temp_path\n",
        "            preview = (transcript[:500] + \"...\") if len(transcript) > 500 else transcript\n",
        "            status_msg = f\"✅ HF Sample: {preview}\"\n",
        "            title = \"HF Sample\"\n",
        "        except Exception as e:\n",
        "            status_msg = f\"❌ HF Import error: {e}\"\n",
        "    else:\n",
        "        status_msg = \"❌ Unsupported source type.\"\n",
        "\n",
        "    return audio_path, transcript, audio_path, status_msg, title\n",
        "\n",
        "# View Full Transcript Helper\n",
        "def show_full_transcript(full_transcript):\n",
        "    return full_transcript or \"❌ No transcript available yet.\"\n"
      ],
      "metadata": {
        "id": "KVAQ6588HDcr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔄 MAS Pipeline Function"
      ],
      "metadata": {
        "id": "UNR5DO9nn9jZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mas_pipeline(uploaded_file, external_audio_path, language=\"en\"):\n",
        "    \"\"\"\n",
        "    Runs transcription and dialect interpretation on the provided audio.\n",
        "    \"\"\"\n",
        "    audio_path = external_audio_path or uploaded_file\n",
        "    # Transcribe with specified language\n",
        "    segments = agent1.transcribe(audio_path, language=language)\n",
        "    results = []\n",
        "    for seg in segments:\n",
        "        # Detect dialect and clarification\n",
        "        dialect, clarification = agent2.detect_dialect(seg[\"text\"])\n",
        "        results.append({\n",
        "            \"Speaker\": seg.get(\"speaker\", \"Speaker\"),\n",
        "            \"Utterance\": seg[\"text\"],\n",
        "            \"Dialect\": dialect,\n",
        "            \"Clarification\": clarification\n",
        "        })\n",
        "    # Return as DataFrame for UI table\n",
        "    return pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "llSyMS1NoB4I"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⚙️ Instantiate All Agents"
      ],
      "metadata": {
        "id": "drUuKCl9Ingw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent1 = AgentInput()\n",
        "agent2 = AgentInterpretation()\n",
        "agent3 = AgentUX()\n",
        "agent4 = AgentTrustLearning(pinata_jwt=PINATA_JWT)"
      ],
      "metadata": {
        "id": "2yaZuJCOaiKq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see what's in my dialect dataset\n"
      ],
      "metadata": {
        "id": "MY3D2tQ20vrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(agent2.df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e682bbd2-32d0-407e-8c89-ddd0b1c7b4b9",
        "id": "ZyCyXWKt05ys",
        "collapsed": true
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                           Utterance           Dialect  \\\n",
            "0          I am seeing her tomorrow.  Nigerian English   \n",
            "1  I didn’t see nobody at the party.  Nigerian English   \n",
            "2               You are coming, abi?  Nigerian English   \n",
            "3               I beg, let's go nau.  Nigerian English   \n",
            "4             I beg, don't be angry.  Nigerian English   \n",
            "\n",
            "                       Clarification  \n",
            "0          I will meet her tomorrow.  \n",
            "1  I didn’t see anyone at the party.  \n",
            "2             You are coming, right?  \n",
            "3  Please, let’s just leave already.  \n",
            "4            Please, don’t be upset.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4c22f45"
      },
      "source": [
        "# 🎛️ Gradio Interface with Import Button"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "bb70c2c8",
        "outputId": "2402ca5d-e5c6-4b6f-a377-fa9c43ee8f0e"
      },
      "source": [
        "def load_external_data(file):\n",
        "    \"\"\"\n",
        "    Load a user-provided dialect CSV into the system.\n",
        "    \"\"\"\n",
        "    if file:\n",
        "        try:\n",
        "            df = pd.read_csv(file.name)\n",
        "            df.to_csv(agent2.data_path, index=False)\n",
        "            agent2._load_data()\n",
        "            return \"✅ Dialect dataset loaded successfully.\"\n",
        "        except Exception as e:\n",
        "            return f\"❌ Failed to load CSV: {e}\"\n",
        "    return \"❌ No file provided.\"\n",
        "\n",
        "with gr.Blocks() as ui:\n",
        "    gr.Markdown(\"## I Understand Understand You - MAS + Blockchain (Beta)\")\n",
        "\n",
        "    # External Import\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"Dialect CSV\"):\n",
        "            csv_file = gr.File(label=\"Upload Dialect CSV\")\n",
        "            csv_msg = gr.Markdown()\n",
        "            csv_file.upload(fn=load_external_data, inputs=[csv_file], outputs=[csv_msg])\n",
        "        with gr.TabItem(\"External Audio\"):\n",
        "            src_type = gr.Radio([\"YouTube\", \"HuggingFace\"], label=\"Source Type\")\n",
        "            src_url = gr.Textbox(label=\"URL\")\n",
        "            sample_count = gr.Number(label=\"HF Samples\", value=1)\n",
        "            import_btn = gr.Button(\"Import Audio\")\n",
        "            ext_audio = gr.Audio(type=\"filepath\", label=\"Imported Audio\", interactive=False)\n",
        "            ext_trans = gr.Textbox(label=\"Transcript Preview\", lines=4)\n",
        "            ext_title = gr.Textbox(label=\"Title\", interactive=False)\n",
        "            import_btn.click(\n",
        "                fn=import_external_audio,\n",
        "                inputs=[src_type, src_url, sample_count],\n",
        "                outputs=[ext_audio, ext_trans, gr.State(), ext_trans, ext_title]\n",
        "            )\n",
        "\n",
        "    # Upload or Record\n",
        "    audio_input = gr.Audio(type=\"filepath\", label=\"Upload/Record Audio\")\n",
        "\n",
        "    # Trim Audio\n",
        "    with gr.Accordion(\"Trim Audio\", open=False):\n",
        "        start_sec = gr.Number(label=\"Start Sec\", value=0)\n",
        "        end_sec = gr.Number(label=\"End Sec\", value=10)\n",
        "        trim_btn = gr.Button(\"Trim\")\n",
        "        trim_btn.click(\n",
        "            fn=agent_input.trim_audio,\n",
        "            inputs=[audio_input, start_sec, end_sec],\n",
        "            outputs=[audio_input]\n",
        "        )\n",
        "\n",
        "    # Analysis\n",
        "    language_dropdown = gr.Dropdown(choices=[\"en\", \"fr\", \"es\", \"de\", \"ko\"], value=\"en\", label=\"Transcription Language\")\n",
        "    analyze_btn = gr.Button(\"Analyze\")\n",
        "    df_out = gr.Dataframe(headers=[\"Speaker\",\"Utterance\",\"Dialect\",\"Clarification\"], interactive=False)\n",
        "    analyze_btn.click(\n",
        "        fn=mas_pipeline,\n",
        "        inputs=[audio_input, ext_audio, language_dropdown],\n",
        "        outputs=[df_out]\n",
        "    )\n",
        "\n",
        "    # Feedback\n",
        "    correction = gr.Textbox(label=\"Suggest Clarification\")\n",
        "    dialect_s = gr.Textbox(label=\"Suggest Dialect\")\n",
        "    fb_btn = gr.Button(\"Submit Feedback\")\n",
        "    fb_status = gr.Markdown()\n",
        "    fb_btn.click(\n",
        "        fn=run_feedback_loop,\n",
        "        inputs=[correction, correction, dialect_s],\n",
        "        outputs=[fb_status]\n",
        "    )\n",
        "\n",
        "    # Download\n",
        "    dl_btn = gr.Button(\"Download Feedback CSV\")\n",
        "    dl_file = gr.File()\n",
        "    dl_btn.click(\n",
        "        fn=download_belief_scores,\n",
        "        outputs=[dl_file]\n",
        "    )\n",
        "\n",
        "    # Clear All Button\n",
        "    clear_btn = gr.Button(\"🧹 Clear All\")\n",
        "    def clear_all():\n",
        "        # Removed ext_status and ext_state as they were not defined and used only internally in import_external_audio\n",
        "        # Also removed ext_state from the return\n",
        "        return None, None, \"\", \"\", [], pd.DataFrame(columns=[\"Speaker\",\"Utterance\",\"Dialect\",\"Clarification\"]), \"\", \"\"\n",
        "    clear_btn.click(fn=clear_all,\n",
        "                    inputs=None,\n",
        "                    outputs=[audio_input, ext_audio, ext_trans, df_out, correction, dialect_s])\n",
        "\n",
        "\n",
        "ui.launch()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://4955be1cbf4702987c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4955be1cbf4702987c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d44d45b"
      },
      "source": [
        "def run_feedback_loop(utterance, clarification_suggestion, dialect_suggestion):\n",
        "    \"\"\"\n",
        "    Processes user feedback and updates the trust and interpretation agents.\n",
        "    \"\"\"\n",
        "    feedback_data = {\n",
        "        \"Utterance\": utterance,\n",
        "        \"SuggestedClarification\": clarification_suggestion,\n",
        "        \"SuggestedDialect\": dialect_suggestion\n",
        "    }\n",
        "    # Assuming the last processed utterance is available\n",
        "    # In a real app, you'd tie feedback to a specific utterance/segment\n",
        "    # For this demo, we'll use the suggestion as a proxy\n",
        "    dialect_identified = dialect_suggestion # Use suggested dialect for belief update\n",
        "    is_positive = True # Assume positive feedback for now, could be refined\n",
        "\n",
        "    cid = agent4.record_feedback(feedback_data, positive=is_positive)\n",
        "    agent4.audit_performance(agent2) # Trigger RLHF update with the feedback\n",
        "\n",
        "    return f\"✅ Feedback recorded. IPFS CID: {cid}\"\n",
        "\n",
        "# This function is a placeholder and should be adapted based on how you want\n",
        "# to download the belief scores dataframe\n",
        "def download_belief_scores():\n",
        "  \"\"\"\n",
        "  Creates a temporary CSV file of the belief scores dataframe for download.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    temp_file = tempfile.NamedTemporaryFile(suffix=\".csv\", delete=False)\n",
        "    agent4.df.to_csv(temp_file.name, index=False)\n",
        "    return temp_file.name\n",
        "  except Exception as e:\n",
        "    print(f\"Error creating download file: {e}\")\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gradio deploy"
      ],
      "metadata": {
        "id": "TEjRQEp0q5yw",
        "outputId": "a6d32279-88ee-4e83-db37-bd5309012e58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Need \u001b[32m'write'\u001b[0m access token to create a Spaces repo.\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "Enter your token (input will not be visible): "
          ]
        }
      ]
    }
  ]
}